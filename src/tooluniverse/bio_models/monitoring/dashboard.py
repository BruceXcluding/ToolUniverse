"""
Bio Modelsç›‘æ§ä»ªè¡¨æ¿
æä¾›å¯è§†åŒ–ç•Œé¢å±•ç¤ºæ¨¡å‹æ€§èƒ½å’Œç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ
"""
import time
import json
from typing import Dict, Any, List, Optional
from datetime import datetime
import structlog

try:
    import streamlit as st
    STREAMLIT_AVAILABLE = True
except ImportError:
    STREAMLIT_AVAILABLE = False

try:
    import plotly.graph_objects as go
    import plotly.express as px
    from plotly.subplots import make_subplots
    PLOTLY_AVAILABLE = True
except ImportError:
    PLOTLY_AVAILABLE = False

try:
    import pandas as pd
    PANDAS_AVAILABLE = True
except ImportError:
    PANDAS_AVAILABLE = False

# Import these inside functions to avoid circular imports
# from . import get_logger, performance_monitor


class MetricsCollector:
    """æŒ‡æ ‡æ”¶é›†å™¨ï¼Œç”¨äºæ”¶é›†å’Œå­˜å‚¨æ€§èƒ½æŒ‡æ ‡"""
    
    def __init__(self):
        self.metrics_history = []
        self.model_metrics = {}
        # å»¶è¿Ÿå¯¼å…¥ä»¥é¿å…å¾ªç¯å¯¼å…¥
        from . import get_logger
        self.logger = get_logger(__name__)
    
    def add_metric(self, metric_name: str, value: float, tags: Dict[str, str] = None) -> None:
        """æ·»åŠ æŒ‡æ ‡"""
        timestamp = datetime.now().isoformat()
        metric_entry = {
            "timestamp": timestamp,
            "name": metric_name,
            "value": value,
            "tags": tags or {}
        }
        self.metrics_history.append(metric_entry)
        
        # é™åˆ¶å†å²è®°å½•æ•°é‡
        if len(self.metrics_history) > 10000:
            self.metrics_history = self.metrics_history[-5000:]
    
    def add_model_metric(self, model_name: str, metric_type: str, value: float) -> None:
        """æ·»åŠ æ¨¡å‹ç‰¹å®šæŒ‡æ ‡"""
        if model_name not in self.model_metrics:
            self.model_metrics[model_name] = {}
        
        if metric_type not in self.model_metrics[model_name]:
            self.model_metrics[model_name][metric_type] = []
        
        self.model_metrics[model_name][metric_type].append({
            "timestamp": datetime.now().isoformat(),
            "value": value
        })
        
        # é™åˆ¶å†å²è®°å½•æ•°é‡
        if len(self.model_metrics[model_name][metric_type]) > 1000:
            self.model_metrics[model_name][metric_type] = self.model_metrics[model_name][metric_type][-500:]
    
    def record_metric(self, metric_name: str, value: float, tags: Dict[str, str] = None) -> None:
        """è®°å½•æŒ‡æ ‡ï¼Œä¸add_metricæ–¹æ³•åŠŸèƒ½ç›¸åŒ"""
        self.add_metric(metric_name, value, tags)
    
    def add_container_metric(self, container_name: str, metric_type: str, value: float) -> None:
        """æ·»åŠ å®¹å™¨ç‰¹å®šæŒ‡æ ‡"""
        metric_name = f"container_{container_name}_{metric_type}"
        self.add_metric(metric_name, value, {"container": container_name, "type": metric_type})
    
    def get_recent_metrics(self, metric_name: str, count: int = 100) -> List[Dict[str, Any]]:
        """è·å–æœ€è¿‘çš„æŒ‡æ ‡"""
        filtered = [m for m in self.metrics_history if m["name"] == metric_name]
        return filtered[-count:] if filtered else []
    
    def get_model_metrics(self, model_name: str, metric_type: str, count: int = 100) -> List[Dict[str, Any]]:
        """è·å–æ¨¡å‹ç‰¹å®šæŒ‡æ ‡"""
        if model_name not in self.model_metrics or metric_type not in self.model_metrics[model_name]:
            return []
        
        return self.model_metrics[model_name][metric_type][-count:] if self.model_metrics[model_name][metric_type] else []
    
    def record_model_success(self, model_name: str, device: str) -> None:
        """è®°å½•æ¨¡å‹åŠ è½½æˆåŠŸ"""
        self.add_metric("model_load_success", 1, {"model_name": model_name, "device": device})
        self.logger.info(f"æ¨¡å‹ {model_name} åŠ è½½æˆåŠŸï¼Œè®¾å¤‡: {device}")
    
    def record_model_failure(self, model_name: str, error_message: str) -> None:
        """è®°å½•æ¨¡å‹åŠ è½½å¤±è´¥"""
        self.add_metric("model_load_failure", 1, {"model_name": model_name, "error": error_message})
        self.logger.error(f"æ¨¡å‹ {model_name} åŠ è½½å¤±è´¥: {error_message}")


# å…¨å±€æŒ‡æ ‡æ”¶é›†å™¨ - å»¶è¿Ÿåˆå§‹åŒ–ä»¥é¿å…å¾ªç¯å¯¼å…¥
metrics_collector = None

def get_metrics_collector():
    """è·å–å…¨å±€æŒ‡æ ‡æ”¶é›†å™¨å®ä¾‹"""
    global metrics_collector
    if metrics_collector is None:
        metrics_collector = MetricsCollector()
    return metrics_collector


def create_dashboard() -> None:
    """åˆ›å»ºStreamlitç›‘æ§ä»ªè¡¨æ¿"""
    if not STREAMLIT_AVAILABLE:
        print("Streamlit not available. Please install it with: pip install streamlit")
        return
    
    st.set_page_config(
        page_title="Bio Modelsç›‘æ§ä»ªè¡¨æ¿",
        page_icon="ğŸ§¬",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    st.title("ğŸ§¬ Bio Modelsç›‘æ§ä»ªè¡¨æ¿")
    
    # ä¾§è¾¹æ 
    st.sidebar.header("æ§åˆ¶é¢æ¿")
    refresh_interval = st.sidebar.slider("åˆ·æ–°é—´éš”(ç§’)", 1, 60, 5)
    
    # ä¸»å†…å®¹åŒºåŸŸ
    tab1, tab2, tab3 = st.tabs(["ç³»ç»Ÿèµ„æº", "æ¨¡å‹æ€§èƒ½", "æ—¥å¿—"])
    
    with tab1:
        display_system_resources()
    
    with tab2:
        display_model_performance()
    
    with tab3:
        display_logs()
    
    # è‡ªåŠ¨åˆ·æ–°
    time.sleep(refresh_interval)
    st.experimental_rerun()


def display_system_resources() -> None:
    """
    æ˜¾ç¤ºç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ
    """
    st.header("ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ")
    
    # å»¶è¿Ÿå¯¼å…¥ä»¥é¿å…å¾ªç¯å¯¼å…¥
    from . import performance_monitor
    from ..model_manager import ModelManager
    
    # è·å–ç³»ç»Ÿèµ„æºä¿¡æ¯
    cpu_memory = performance_monitor.get_cpu_memory_info()
    cpu_usage = performance_monitor.get_cpu_usage()
    
    # åˆ›å»ºæ¨¡å‹ç®¡ç†å™¨å®ä¾‹ä»¥è·å–æ¨¡å‹ä¿¡æ¯
    try:
        model_manager = ModelManager()
        loaded_models = model_manager.list_loaded_models()
        model_names = [model['name'] for model in loaded_models if model['type'] != 'container']
    except Exception:
        model_names = []
    
    # CPUå’Œå†…å­˜ä¿¡æ¯
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("CPUä½¿ç”¨ç‡")
        if PLOTLY_AVAILABLE:
            fig = go.Figure(go.Indicator(
                mode="gauge+number+delta",
                value=cpu_usage,
                domain={'x': [0, 1], 'y': [0, 1]},
                title={'text': "CPU (%)"},
                delta={'reference': 50},
                gauge={
                    'axis': {'range': [None, 100]},
                    'bar': {'color': "darkblue"},
                    'steps': [
                        {'range': [0, 50], 'color': "lightgray"},
                        {'range': [50, 80], 'color': "yellow"},
                        {'range': [80, 100], 'color': "red"}
                    ],
                    'threshold': {
                        'line': {'color': "red", 'width': 4},
                        'thickness': 0.75,
                        'value': 90
                    }
                }
            ))
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.metric("CPUä½¿ç”¨ç‡", f"{cpu_usage:.2f}%")
    
    with col2:
        st.subheader("å†…å­˜ä½¿ç”¨æƒ…å†µ")
        memory_percent = cpu_memory["percent"]
        if PLOTLY_AVAILABLE:
            fig = go.Figure(go.Indicator(
                mode="gauge+number+delta",
                value=memory_percent,
                domain={'x': [0, 1], 'y': [0, 1]},
                title={'text': "Memory (%)"},
                delta={'reference': 50},
                gauge={
                    'axis': {'range': [None, 100]},
                    'bar': {'color': "darkgreen"},
                    'steps': [
                        {'range': [0, 50], 'color': "lightgray"},
                        {'range': [50, 80], 'color': "yellow"},
                        {'range': [80, 100], 'color': "red"}
                    ],
                    'threshold': {
                        'line': {'color': "red", 'width': 4},
                        'thickness': 0.75,
                        'value': 90
                    }
                }
            ))
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.metric("å†…å­˜ä½¿ç”¨ç‡", f"{memory_percent:.2f}%")
            st.metric("å·²ç”¨å†…å­˜", f"{cpu_memory['used'] / (1024**3):.2f} GB")
            st.metric("æ€»å†…å­˜", f"{cpu_memory['total'] / (1024**3):.2f} GB")
    
    # GPUä¿¡æ¯
    if performance_monitor.enable_gpu_monitoring:
        st.subheader("GPUä½¿ç”¨æƒ…å†µ")
        
        # æ·»åŠ æ¨¡å‹ç­›é€‰é€‰é¡¹
        model_filter = st.selectbox(
            "é€‰æ‹©æ¨¡å‹ç­›é€‰ï¼ˆç©ºè¡¨ç¤ºæ˜¾ç¤ºæ‰€æœ‰æ¨¡å‹ä½¿ç”¨çš„GPUï¼‰", 
            options=[""] + model_names,
            format_func=lambda x: "å…¨éƒ¨æ¨¡å‹" if x == "" else x
        )
        
        # è·å–éœ€è¦æ˜¾ç¤ºçš„GPU IDåˆ—è¡¨
        gpu_ids_to_display = set()
        
        if model_filter:
            # å¦‚æœé€‰æ‹©äº†ç‰¹å®šæ¨¡å‹ï¼Œå°è¯•è·å–è¯¥æ¨¡å‹ä½¿ç”¨çš„GPU
            try:
                # è·å–æ¨¡å‹ä½¿ç”¨çš„GPUä¿¡æ¯
                model_gpu_info = model_manager.gpu_scheduler.get_model_gpu_info(model_filter)
                if model_gpu_info:
                    # ä»cudaè®¾å¤‡å­—ç¬¦ä¸²ä¸­æå–GPU ID
                    gpu_id = int(model_gpu_info.split(':')[-1])
                    gpu_ids_to_display.add(gpu_id)
                else:
                    st.info(f"æ¨¡å‹ {model_filter} æœªä½¿ç”¨GPUæˆ–æœªåŠ è½½")
            except Exception as e:
                st.warning(f"è·å–æ¨¡å‹GPUä¿¡æ¯å¤±è´¥: {e}")
                # å¦‚æœè·å–å¤±è´¥ï¼Œæ˜¾ç¤ºæ‰€æœ‰æ´»è·ƒGPUä½œä¸ºå¤‡é€‰
                for gpu_id in range(performance_monitor.gpu_count):
                    gpu_memory = performance_monitor.get_gpu_memory_info(gpu_id)
                    if gpu_memory["allocated"] > 0:
                        gpu_ids_to_display.add(gpu_id)
        else:
            # é»˜è®¤æ˜¾ç¤ºæ‰€æœ‰æœ‰æ˜¾å­˜ä½¿ç”¨çš„GPU
            for gpu_id in range(performance_monitor.gpu_count):
                gpu_memory = performance_monitor.get_gpu_memory_info(gpu_id)
                if gpu_memory["allocated"] > 0:
                    gpu_ids_to_display.add(gpu_id)
        
        # æ˜¾ç¤ºé€‰ä¸­çš„GPUä¿¡æ¯
        active_gpus = []
        for gpu_id in sorted(gpu_ids_to_display):
            gpu_memory = performance_monitor.get_gpu_memory_info(gpu_id)
            active_gpus.append((gpu_id, gpu_memory))
        
        if not active_gpus:
            st.info("å½“å‰æ²¡æœ‰GPUæ­£åœ¨ä½¿ç”¨")
        else:
            # æ˜¾ç¤ºGPUä¿¡æ¯ï¼Œå¹¶æ ‡æ³¨ä½¿ç”¨è¯¥GPUçš„æ¨¡å‹
            gpu_cols = st.columns(min(len(active_gpus), 4))
            for idx, (gpu_id, gpu_memory) in enumerate(active_gpus):
                with gpu_cols[idx % 4]:
                    memory_percent = (gpu_memory["allocated"] / gpu_memory["total"]) * 100 if gpu_memory["total"] > 0 else 0
                    
                    # å°è¯•è·å–ä½¿ç”¨è¯¥GPUçš„æ¨¡å‹
                    using_models = []
                    try:
                        models_on_gpu = model_manager.gpu_scheduler.get_gpu_models_info(f"cuda:{gpu_id}")
                        if models_on_gpu:
                            using_models = models_on_gpu
                    except Exception:
                        pass
                    
                    # æ˜¾ç¤ºGPUä¿¡æ¯
                    st.metric(f"GPU {gpu_id}", f"{memory_percent:.2f}%")
                    st.metric("å·²ç”¨æ˜¾å­˜", f"{gpu_memory['allocated'] / (1024**3):.2f} GB")
                    st.metric("æ€»æ˜¾å­˜", f"{gpu_memory['total'] / (1024**3):.2f} GB")
                    
                    # æ˜¾ç¤ºä½¿ç”¨è¯¥GPUçš„æ¨¡å‹
                    if using_models:
                        st.text("ä½¿ç”¨ä¸­çš„æ¨¡å‹:")
                        for model in using_models:
                            st.text(f"  - {model}")
        
        # PyTorch GPUå†…å­˜
        torch_memory = performance_monitor.get_torch_gpu_memory_info()
        if torch_memory["allocated"] > 0 or torch_memory["reserved"] > 0:
            st.subheader("PyTorch GPUå†…å­˜")
            col1, col2 = st.columns(2)
            with col1:
                st.metric("å·²åˆ†é…", f"{torch_memory['allocated']:.2f} GB")
            with col2:
                st.metric("å·²ä¿ç•™", f"{torch_memory['reserved']:.2f} GB")


def display_model_performance() -> None:
    """æ˜¾ç¤ºæ¨¡å‹æ€§èƒ½æŒ‡æ ‡"""
    st.header("æ¨¡å‹æ€§èƒ½æŒ‡æ ‡")
    
    # è·å–æŒ‡æ ‡æ”¶é›†å™¨å®ä¾‹
    collector = get_metrics_collector()
    
    if not collector.model_metrics:
        st.info("æš‚æ— æ¨¡å‹æ€§èƒ½æ•°æ®")
        return
    
    # æ¨¡å‹é€‰æ‹©
    model_names = list(collector.model_metrics.keys())
    selected_model = st.selectbox("é€‰æ‹©æ¨¡å‹", model_names)
    
    if selected_model:
        model_data = collector.model_metrics[selected_model]
        
        # æ˜¾ç¤ºä¸åŒç±»å‹çš„æŒ‡æ ‡
        metric_types = list(model_data.keys())
        selected_metric = st.selectbox("é€‰æ‹©æŒ‡æ ‡ç±»å‹", metric_types)
        
        if selected_metric and model_data[selected_metric]:
            # å‡†å¤‡æ•°æ®
            timestamps = [m["timestamp"] for m in model_data[selected_metric]]
            values = [m["value"] for m in model_data[selected_metric]]
            
            # åˆ›å»ºå›¾è¡¨
            if PLOTLY_AVAILABLE and PANDAS_AVAILABLE:
                df = pd.DataFrame({
                    "timestamp": timestamps,
                    "value": values
                })
                df["timestamp"] = pd.to_datetime(df["timestamp"])
                
                fig = px.line(
                    df, 
                    x="timestamp", 
                    y="value",
                    title=f"{selected_model} - {selected_metric}",
                    labels={"value": selected_metric, "timestamp": "æ—¶é—´"}
                )
                st.plotly_chart(fig, use_container_width=True)
            else:
                # ç®€å•çš„æ–‡æœ¬æ˜¾ç¤º
                latest_value = values[-1] if values else 0
                st.metric(f"æœ€æ–°{selected_metric}", f"{latest_value:.4f}")
                
                # æ˜¾ç¤ºå†å²å€¼
                st.subheader("å†å²å€¼")
                for i, (ts, val) in enumerate(zip(timestamps[-10:], values[-10:])):
                    st.write(f"{ts}: {val:.4f}")


def display_logs() -> None:
    """æ˜¾ç¤ºæ—¥å¿—ä¿¡æ¯"""
    st.header("æ—¥å¿—ä¿¡æ¯")
    
    # æ—¥å¿—çº§åˆ«é€‰æ‹©
    log_level = st.selectbox("é€‰æ‹©æ—¥å¿—çº§åˆ«", ["INFO", "WARNING", "ERROR", "DEBUG"])
    
    # æ—¥å¿—æ•°é‡é™åˆ¶
    log_limit = st.slider("æ˜¾ç¤ºæ—¥å¿—æ•°é‡", 10, 1000, 100)
    
    # è¿™é‡Œåº”è¯¥ä»å®é™…çš„æ—¥å¿—å­˜å‚¨ä¸­è·å–æ—¥å¿—
    # ç”±äºæˆ‘ä»¬æ²¡æœ‰å®ç°æ—¥å¿—å­˜å‚¨ï¼Œè¿™é‡Œåªæ˜¯ç¤ºä¾‹
    st.info("æ—¥å¿—æ˜¾ç¤ºåŠŸèƒ½éœ€è¦ä¸å®é™…çš„æ—¥å¿—å­˜å‚¨ç³»ç»Ÿé›†æˆ")
    
    # ç¤ºä¾‹æ—¥å¿—æ•°æ®
    example_logs = [
        {"timestamp": datetime.now().isoformat(), "level": "INFO", "message": "æ¨¡å‹åŠ è½½å®Œæˆ", "model": "DNABERT-2"},
        {"timestamp": datetime.now().isoformat(), "level": "INFO", "message": "æ¨ç†å¼€å§‹", "model": "LucaOne"},
        {"timestamp": datetime.now().isoformat(), "level": "WARNING", "message": "GPUå†…å­˜ä½¿ç”¨ç‡é«˜", "gpu_id": 0, "usage": "85%"},
        {"timestamp": datetime.now().isoformat(), "level": "INFO", "message": "æ¨ç†å®Œæˆ", "model": "AlphaFold", "time": "2.3s"},
    ]
    
    # æ˜¾ç¤ºæ—¥å¿—
    for log in example_logs[:log_limit]:
        if log["level"] == log_level or log_level == "INFO":
            with st.expander(f"{log['timestamp']} - {log['level']} - {log['message']}"):
                st.json(log)


def run_dashboard(host: str = "localhost", port: int = 8501) -> None:
    """è¿è¡Œç›‘æ§ä»ªè¡¨æ¿"""
    if not STREAMLIT_AVAILABLE:
        print("Streamlit not available. Please install it with: pip install streamlit")
        return
    
    try:
        import subprocess
        import os
        
        # è·å–å½“å‰è„šæœ¬è·¯å¾„
        script_path = os.path.abspath(__file__)
        
        # è¿è¡Œstreamlit
        cmd = [
            "streamlit", "run", script_path,
            "--server.address", host,
            "--server.port", str(port)
        ]
        
        print(f"Starting dashboard at http://{host}:{port}")
        subprocess.run(cmd)
    except Exception as e:
        print(f"Failed to start dashboard: {e}")


if __name__ == "__main__":
    # å¦‚æœç›´æ¥è¿è¡Œæ­¤è„šæœ¬ï¼Œå¯åŠ¨ä»ªè¡¨æ¿
    run_dashboard()